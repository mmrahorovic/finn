{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Convolution: https://medium.com/analytics-vidhya/2d-convolution-using-python-numpy-43442ff5f381\n",
    "# Relu: out * (out > 0)\n",
    "def expected_output(x, w, padding=0, strides=1):\n",
    "    x_h = x.shape[2]\n",
    "    x_w = x.shape[3]\n",
    "    w_h = w.shape[2]\n",
    "    w_w = w.shape[3]    \n",
    "    y_w = int ((x_w - w_w + 2*padding)/(strides) + 1)\n",
    "    y_h = int ((x_h - w_h + 2*padding)/(strides) + 1)\n",
    "    y = np.zeros((y_h,y_w))\n",
    "    \n",
    "    im_padded = np.pad(x, padding,\n",
    "                       mode='constant',\n",
    "                       constant_values=(0))\n",
    "    im_padded = np.reshape(im_padded,[x_h,x_w])\n",
    "    w = np.reshape(w, [w_h,w_w])\n",
    "    #print(\"{} \\n {}\" .format(im_padded, w))\n",
    "    width_lim = x_w-w_w+1\n",
    "    height_lim = x_h-w_h+1\n",
    "    for ver in range(0, height_lim, strides):\n",
    "        for hor in range(0, width_lim, strides):\n",
    "            y[ver,hor] = (w*im_padded[ver:ver+w_h, hor:hor+w_w]).sum()\n",
    "    y = np.reshape(y,[1,1,y.shape[0],y.shape[1]]).astype(np.float32)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_np: (1, 3, 1, 5)\n",
      "[[[[0. 1. 2. 3. 4.]]\n",
      "\n",
      "  [[0. 1. 2. 3. 4.]]\n",
      "\n",
      "  [[0. 1. 2. 3. 4.]]]]\n",
      "W_np: (1, 3, 1, 3)\n",
      "[[[[1. 1. 1.]]\n",
      "\n",
      "  [[1. 1. 1.]]\n",
      "\n",
      "  [[1. 1. 1.]]]]\n",
      "y_np: (1, 1, 1, 3)\n",
      "[[[[ 9. 18. 27.]]]]\n",
      "Serving '/tmp/convolution_node.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://0.0.0.0:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc2d8676550>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.util.visualization import showInNetron\n",
    "    \n",
    "import onnx\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "\n",
    "# Input/output has shape NCHW\n",
    "#x_np = np.array([[[[0., 1., 2., 3., 4.],  # (1, 1, 5, 5) input tensor\n",
    "#                [5., 6., 7., 8., 9.],\n",
    "#                [10., 11., 12., 13., 14.],\n",
    "#                [15., 16., 17., 18., 19.],\n",
    "#                [20., 21., 22., 23., 24.]]]]).astype(np.float32)\n",
    "\n",
    "#W_np = np.array([[[[1., 1., 1.],  # (1, 1, 3, 3) tensor for convolution weights\n",
    "#                [1., 1., 1.],\n",
    "#                [1., 1., 1.]]]]).astype(np.float32)\n",
    "\n",
    "#y_np = np.array([[[[54., 63., 72.],  # (1, 1, 3, 3) output tensor\n",
    "#                [99., 108., 117.],\n",
    "#                [144., 153., 162.]]]]).astype(np.float32)\n",
    "\n",
    "#x_np = np.array(\n",
    "#[\n",
    "#    [\n",
    "#        [[0., 1., 2., 3., 4.],  # (1, 1, 5, 5) input tensor\n",
    "#         [5., 6., 7., 8., 9.],\n",
    "#         [10., 11., 12., 13., 14.],\n",
    "#         [15., 16., 17., 18., 19.],\n",
    "#         [20., 21., 22., 23., 24.]\n",
    "#        ],\n",
    "#        [[0., 1., 2., 3., 4.],  # (1, 1, 5, 5) input tensor\n",
    "#         [5., 6., 7., 8., 9.],\n",
    "#         [10., 11., 12., 13., 14.],\n",
    "#         [15., 16., 17., 18., 19.],\n",
    "#         [20., 21., 22., 23., 24.]\n",
    "#        ],\n",
    "#        [[0., 1., 2., 3., 4.],  # (1, 1, 5, 5) input tensor\n",
    "#         [5., 6., 7., 8., 9.],\n",
    "#         [10., 11., 12., 13., 14.],\n",
    "#         [15., 16., 17., 18., 19.],\n",
    "#         [20., 21., 22., 23., 24.]\n",
    "#        ]        \n",
    "#    ]\n",
    "#]).astype(np.float32)\n",
    "\n",
    "# (1, 1, 3, 3) tensor for convolution weights\n",
    "#W_np = np.array(\n",
    "#[\n",
    "#    [\n",
    "#        [[1.,1.,1.],\n",
    "#         [1.,1.,1.],\n",
    "#         [1.,1.,1.]\n",
    "#        ],\n",
    "#        [[1.,1.,1.],\n",
    "#         [1.,1.,1.],\n",
    "#         [1.,1.,1.]\n",
    "#        ],\n",
    "#        [[1.,1.,1.],\n",
    "#         [1.,1.,1.],\n",
    "#         [1.,1.,1.]\n",
    "#        ]\n",
    "#    ]\n",
    "#]    ).astype(np.float32)\n",
    "\n",
    "# y_np: Batch size x OFM (num of output channels) x O_H x O_W\n",
    "#y_np = expected_output(x_np, W_np, padding=0, strides=1)\n",
    "#y_np = np.array(\n",
    "#[\n",
    "#    [\n",
    "#        [[162.,189.,216.],\n",
    "#         [297.,324.,351.],\n",
    "#         [432.,459.,486.]\n",
    "#        ]\n",
    "#    ]\n",
    "#]).astype(np.float32)\n",
    "\n",
    "x_np = np.array(\n",
    "[\n",
    "    [\n",
    "        [[0., 1., 2., 3., 4.],  # (1, 1, 5, 5) input tensor\n",
    "        ],\n",
    "        [[0., 1., 2., 3., 4.],  # (1, 1, 5, 5) input tensor\n",
    "        ],\n",
    "        [[0., 1., 2., 3., 4.],  # (1, 1, 5, 5) input tensor\n",
    "        ]        \n",
    "    ]\n",
    "]).astype(np.float32)\n",
    "\n",
    "# (1, 1, 3, 3) tensor for convolution weights\n",
    "W_np = np.array(\n",
    "[\n",
    "    [\n",
    "        [[1.,1.,1.],\n",
    "        ],\n",
    "        [[1.,1.,1.],\n",
    "        ],\n",
    "        [[1.,1.,1.],\n",
    "        ]\n",
    "    ]\n",
    "]    ).astype(np.float32)\n",
    "\n",
    "# y_np: Batch size x OFM (num of output channels) x O_H x O_W\n",
    "#y_np = expected_output(x_np, W_np, padding=0, strides=1)\n",
    "y_np = np.array(\n",
    "[\n",
    "    [\n",
    "        [[9.,18.,27.],\n",
    "        ]\n",
    "    ]\n",
    "]).astype(np.float32)\n",
    "\n",
    "\n",
    "print(\"x_np: {}\\n{}\" .format(x_np.shape,x_np))\n",
    "print(\"W_np: {}\\n{}\" .format(W_np.shape,W_np))\n",
    "print(\"y_np: {}\\n{}\" .format(y_np.shape,y_np))\n",
    "\n",
    "x = onnx.helper.make_tensor_value_info(\"x\", onnx.TensorProto.FLOAT, x_np.shape)\n",
    "W = onnx.helper.make_tensor_value_info(\"W\", onnx.TensorProto.FLOAT, W_np.shape)\n",
    "y = onnx.helper.make_tensor_value_info(\"y\", onnx.TensorProto.FLOAT, y_np.shape)\n",
    "\n",
    "conv_node = onnx.helper.make_node(\n",
    "    'Conv',\n",
    "    inputs = ['x','W'],\n",
    "    outputs = ['y'],\n",
    "    kernel_shape = [W_np.shape[2],W_np.shape[3]],\n",
    "    #auto_pad = 'NOTSET',\n",
    "    dilations = [1,1],\n",
    "    group = 1,\n",
    "    pads = [0,0,0,0],\n",
    "    strides = [1,1]\n",
    ")\n",
    "\n",
    "graph = onnx.helper.make_graph(\n",
    "    nodes=[\n",
    "        conv_node\n",
    "    ],\n",
    "    name=\"Lowering convolution\",\n",
    "    inputs=[x],\n",
    "    outputs=[y]\n",
    ")\n",
    "\n",
    "onnx_model = onnx.helper.make_model(graph, producer_name=\"convolution-node\")\n",
    "onnx.save(onnx_model, '/tmp/convolution_node.onnx')\n",
    "\n",
    "showInNetron('/tmp/convolution_node.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/tmp/convolution_node_cleaned.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://0.0.0.0:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc30837d128>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tidy-up\n",
    "from finn.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from finn.transformation.infer_shapes import InferShapes\n",
    "from finn.transformation.infer_datatypes import InferDataTypes\n",
    "from finn.transformation.fold_constants import FoldConstants\n",
    "\n",
    "## Apply transformation\n",
    "from finn.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "\n",
    "model = ModelWrapper(onnx_model)\n",
    "model.set_initializer('W',W_np)\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "\n",
    "model.save('/tmp/convolution_node_cleaned.onnx')\n",
    "showInNetron('/tmp/convolution_node_cleaned.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W_matmul dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 3, 2)\n",
      "(2, 5)\n",
      "[[[[5 5]\n",
      "   [5 5]\n",
      "   [5 5]]\n",
      "\n",
      "  [[2 2]\n",
      "   [2 2]\n",
      "   [2 2]]\n",
      "\n",
      "  [[3 3]\n",
      "   [3 3]\n",
      "   [3 3]]]]\n",
      "[[1 1 1 1 1]\n",
      " [1 1 1 1 1]]\n",
      "[[[[10 10 10 10 10]\n",
      "   [10 10 10 10 10]\n",
      "   [10 10 10 10 10]]\n",
      "\n",
      "  [[ 4  4  4  4  4]\n",
      "   [ 4  4  4  4  4]\n",
      "   [ 4  4  4  4  4]]\n",
      "\n",
      "  [[ 6  6  6  6  6]\n",
      "   [ 6  6  6  6  6]\n",
      "   [ 6  6  6  6  6]]]]\n",
      "(1, 3, 3, 5)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "a = np.array([\n",
    "    [\n",
    "        [[5,5],[5,5],[5,5]]\n",
    "    ,\n",
    "        [[2,2],[2,2],[2,2]]\n",
    "    ,\n",
    "        [[3,3],[3,3],[3,3]]\n",
    "    ]\n",
    "])\n",
    "\n",
    "b = np.array(\n",
    "[\n",
    "    [1,1,1,1,1],\n",
    "    [1,1,1,1,1]\n",
    "])\n",
    "\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "print(a)\n",
    "print(b)\n",
    "c=np.matmul(a,b)\n",
    "print(c)\n",
    "print(c.shape)\n",
    "\n",
    "d=np.array([3,1])\n",
    "print(type(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_conv (reading out):\n",
      "(1, 3, 1, 3)\n",
      "[[[[1. 1. 1.]]\n",
      "\n",
      "  [[1. 1. 1.]]\n",
      "\n",
      "  [[1. 1. 1.]]]]\n",
      "ofm_dim: [1, 1, 1, 3]\n",
      "W_matmul:\n",
      "(1, 1, 3, 3)\n",
      "[[[[1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]]]]\n",
      "W_matmul:\n",
      "(1, 9)\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "W_matmul:\n",
      "(9, 1)\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "from finn.util.basic import get_by_name\n",
    "\n",
    "node_ind=0\n",
    "graph_modified=False\n",
    "for n in model.graph.node:\n",
    "    node_ind += 1\n",
    "    if n.op_type == \"Conv\":\n",
    "        graph_modified = True\n",
    "        cnv_input = n.input[0]\n",
    "        cnv_output = n.output[0]\n",
    "        idt = model.get_tensor_datatype(cnv_input)\n",
    "        odt = model.get_tensor_datatype(cnv_output)\n",
    "        # extract conv parameters\n",
    "        ###\n",
    "        #k = get_by_name(n.attribute, \"kernel_shape\").ints[-1]\n",
    "        k_H = get_by_name(n.attribute, \"kernel_shape\").ints[0]\n",
    "        k_W = get_by_name(n.attribute, \"kernel_shape\").ints[1]\n",
    "        ###\n",
    "        pad = get_by_name(n.attribute, \"pads\").ints[-1]\n",
    "        stride = get_by_name(n.attribute, \"strides\").ints[-1]\n",
    "        group = get_by_name(n.attribute, \"group\").i\n",
    "        weight_name = n.input[1]\n",
    "        W_conv = model.get_initializer(weight_name)\n",
    "        print(\"W_conv (reading out):\\n{}\\n{}\".format(W_conv.shape,W_conv))\n",
    "        ifm_ch = model.get_tensor_shape(n.input[0])[1]  # assume NCHW\n",
    "        ofm_ch = model.get_tensor_shape(n.output[0])[1]  # assume NCHW\n",
    "        ifm_dim_H = model.get_tensor_shape(n.input[0])[2]  # assume NCHW\n",
    "        ifm_dim_W = model.get_tensor_shape(n.input[0])[3]  # assume NCHW\n",
    "        ofm_dim_H = model.get_tensor_shape(n.output[0])[2]  # assume NCHW\n",
    "        ofm_dim_W = model.get_tensor_shape(n.output[0])[3]  # assume NCHW\n",
    "        print(\"ofm_dim: {}\" .format(model.get_tensor_shape(n.output[0])))\n",
    "        \n",
    "        # if depthwise conv create sparse matrix and variable \"dw\"\n",
    "        # to store as attribute in Im2Col that indicates that the created\n",
    "        # Im2Col node belongs to a depthwise convolution\n",
    "        dw = False\n",
    "        if group == ifm_ch and ofm_ch == ifm_ch:\n",
    "            \n",
    "            ###\n",
    "            W_sparse = np.zeros((ofm_ch, ifm_ch, k_H, k_W))\n",
    "            ###\n",
    "            \n",
    "            print(\"W_sparse:\\n{}\\n{}\".format(W_sparse.shape,W_sparse))\n",
    "            print(\"W_conv:\\n{}\\n{}\".format(W_conv.shape,W_conv))\n",
    "            for ch in range(ifm_ch):\n",
    "                W_sparse[ch][ch] = W_conv[ch][0]\n",
    "            print(\"W_sparse:\\n{}\\n{}\".format(W_sparse.shape,W_sparse))\n",
    "            W_conv = W_sparse.astype(np.float32)\n",
    "            # we need to store information of the\n",
    "            # sparsity of the weight matrix. For this\n",
    "            # we use the sparsity annotation of the\n",
    "            # weight tensor\n",
    "            \n",
    "            ###\n",
    "            sparsity = {\"dw\": {\"kernel_shape\": k_H}}\n",
    "            ### MUST CHANGE IN OTHER FILES TOO (how it is used)\n",
    "            \n",
    "            model.set_tensor_sparsity(weight_name, sparsity)\n",
    "            # additionally create variable \"dw\" to store\n",
    "            # as attribute in Im2Col that indicates that the created\n",
    "            # Im2Col node belongs to a depthwise convolution\n",
    "            dw = True\n",
    "        # reuse conv weights for new matmul weights\n",
    "        # conv weights are [OFM][IFM][k][k]\n",
    "        # first convert to [OFM][k][k][IFM] (to remain compatible with\n",
    "        # finn-hlslib and how it does im2col/sliding window)\n",
    "        W_matmul = W_conv.transpose(0, 2, 3, 1)\n",
    "        print(\"W_matmul:\\n{}\\n{}\".format(W_matmul.shape,W_matmul))\n",
    "        # reshape into [OFM][k*k*IFM] matrix\n",
    "        \n",
    "        ###\n",
    "        W_matmul = W_matmul.reshape(ofm_ch, ifm_ch * k_H * k_W)\n",
    "        ###\n",
    "        \n",
    "        print(\"W_matmul:\\n{}\\n{}\".format(W_matmul.shape,W_matmul))\n",
    "        # transpose to get ONNX-compatible [k*k*IFM][OFM] matrix\n",
    "        W_matmul = W_matmul.T\n",
    "        print(\"W_matmul:\\n{}\\n{}\".format(W_matmul.shape,W_matmul))\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding\n",
    "What is it used for? How to change this part accordingly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[1, 2, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "def _auto_pad_to_explicit_padding(autopad_str, idim, k, stride, n_dims):\n",
    "    pad_total = (stride - 1) * idim - stride + k\n",
    "    pad_half_small = int((pad_total / 2))\n",
    "    pad_half_large = pad_total - pad_half_small\n",
    "    if autopad_str == \"VALID\":\n",
    "        return [0 for i in range(2 * n_dims)]\n",
    "    elif autopad_str == \"SAME_UPPER\":\n",
    "        return [pad_half_small, pad_half_large] * n_dims\n",
    "    elif autopad_str == \"SAME_LOWER\":\n",
    "        return [pad_half_large, pad_half_small] * n_dims\n",
    "    else:\n",
    "        raise Exception(\"Unsupported auto_pad: \" + autopad_str)\n",
    "        \n",
    "for n in model.graph.node:\n",
    "    auto_pad = get_by_name(n.attribute, \"auto_pad\")\n",
    "    print(auto_pad)\n",
    "    pad = _auto_pad_to_explicit_padding(\n",
    "        \"SAME_UPPER\",\n",
    "        idim=5,\n",
    "        k=4,\n",
    "        stride=1,\n",
    "        n_dims=len(model.get_tensor_shape(n.input[0]))-2\n",
    "    )\n",
    "    print(pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply LowerConvsToMatMul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/tmp/lowered_convolution_node.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://0.0.0.0:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc2d6952668>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModelWrapper('/tmp/convolution_node_cleaned.onnx')\n",
    "model = model.transform(LowerConvsToMatMul())\n",
    "\n",
    "model.save('/tmp/lowered_convolution_node.onnx')\n",
    "showInNetron('/tmp/lowered_convolution_node.onnx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get_im2col_indices_nchw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1 5\n",
      "i0: [0 0 0]\n",
      "i0: [0 0 0]\n",
      "i1: [0 0 0]\n",
      "j0: [0 1 2]\n",
      "j1: [0 1 2]\n",
      "i0.reshape(-1,1): [[0]\n",
      " [0]\n",
      " [0]]\n",
      "i1.reshape(1,-1): [[0 0 0]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "j0.reshape(-1,1): [[0]\n",
      " [1]\n",
      " [2]]\n",
      "j1.reshape(1,-1): [[0 1 2]]\n",
      "[[0 1 2]\n",
      " [1 2 3]\n",
      " [2 3 4]]\n",
      "\n",
      "\n",
      "Cols:\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "def compute_conv_output_dim(ifm_dim, k, stride, pad=0):\n",
    "    \"\"\"Returns spatial output dimension size for convolution with given params.\"\"\"\n",
    "    return int(((ifm_dim + 2 * pad - k) / stride) + 1)\n",
    "\n",
    "def get_im2col_indices_nchw(\n",
    "    x_shape, field_height, field_width, padding=0, stride_y=1, stride_x=1\n",
    "):\n",
    "    \"\"\"Returns im2col indices.\"\"\"\n",
    "    # First figure out what the size of the output should be\n",
    "    N, C, H, W = x_shape\n",
    "    print(\"{} {} {} {}\" .format(N,C,H,W))\n",
    "    out_height = compute_conv_output_dim(H, field_height, stride_y, padding)\n",
    "    out_width = compute_conv_output_dim(W, field_width, stride_x, padding)\n",
    "    \n",
    "    i0 = np.repeat(np.arange(field_height), field_width)\n",
    "    print(\"i0: {}\".format(i0))\n",
    "    i0 = np.tile(i0, C)\n",
    "    print(\"i0: {}\".format(i0))\n",
    "    i1 = stride_y * np.repeat(np.arange(out_height), out_width)\n",
    "    print(\"i1: {}\".format(i1))\n",
    "    j0 = np.tile(np.arange(field_width), field_height * C)\n",
    "    print(\"j0: {}\".format(j0))\n",
    "    j1 = stride_x * np.tile(np.arange(out_width), out_height)\n",
    "    print(\"j1: {}\".format(j1))\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "    print(\"i0.reshape(-1,1): {}\".format(i0.reshape(-1,1)))\n",
    "    print(\"i1.reshape(1,-1): {}\".format(i1.reshape(1,-1)))\n",
    "    print(\"{}\" .format(i))\n",
    "    print(\"j0.reshape(-1,1): {}\".format(j0.reshape(-1,1)))\n",
    "    print(\"j1.reshape(1,-1): {}\".format(j1.reshape(1,-1)))\n",
    "    print(\"{}\" .format(j))\n",
    "    #print(j)\n",
    "    \n",
    "    k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n",
    "    #print(k)\n",
    "    \n",
    "    return (k, i, j)\n",
    "\n",
    "#x_np = np.array([[[[0., 1., 2., 3., 4.],  # (1, 1, 5, 5) input tensor\n",
    "#                ]]]).astype(np.float32)\n",
    "\n",
    "kernel_height=1\n",
    "kernel_width=3\n",
    "k,i,j = get_im2col_indices_nchw(x_np.shape,kernel_height,kernel_width)\n",
    "\n",
    "cols = x_np[:, k, i, j]\n",
    "C = x_np.shape[1]\n",
    "cols = cols.transpose(1,2,0).reshape(3*3*C,-1)\n",
    "\n",
    "print(\"\\n\\nCols:\\n{}\" .format(cols))\n",
    "\n",
    "#print(x_np[:,0,[[0,0],[4,4]],[[0,4],[0,4]]])\n",
    "# Input/output has shape NCHW\n",
    "#x_np = np.array([[[[0., 1., 2., 3., 4.],  # (1, 1, 5, 5) input tensor\n",
    "#                [5., 6., 7., 8., 9.],\n",
    "#                [10., 11., 12., 13., 14.],\n",
    "#                [15., 16., 17., 18., 19.],\n",
    "#                [20., 21., 22., 23., 24.]]]]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      " [[0 0 0 1 1 1 2 2 2 3 3 3 4 4 4]\n",
      " [0 0 0 1 1 1 2 2 2 3 3 3 4 4 4]\n",
      " [0 0 0 1 1 1 2 2 2 3 3 3 4 4 4]]\n",
      "\n",
      " [[0 1 2 0 1 2 0 1 2 0 1 2 0 1 2]\n",
      " [1 2 3 1 2 3 1 2 3 1 2 3 1 2 3]\n",
      " [2 3 4 2 3 4 2 3 4 2 3 4 2 3 4]]\n"
     ]
    }
   ],
   "source": [
    "print(\"{}\\n\\n {}\\n\\n {}\" .format(k,i,j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test original CONV and reduced CONV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected output:\n",
      "[[[[ 9. 18. 27.]]]]\n",
      "Conv node:\n",
      "Test passed\n",
      "Lowered conv node:\n",
      "Test passed\n"
     ]
    }
   ],
   "source": [
    "import finn.core.onnx_exec as oxe\n",
    "\n",
    "print(\"Expected output:\\n{}\".format(y_np))\n",
    "\n",
    "model = ModelWrapper('/tmp/convolution_node_cleaned.onnx')\n",
    "input_dict = {\"global_in\": x_np}\n",
    "y_produced = oxe.execute_onnx(model, input_dict)[\"global_out\"]\n",
    "print(\"Conv node:\")\n",
    "if (y_produced==y_np).all():\n",
    "    print(\"Test passed\")\n",
    "else:\n",
    "    print(\"Test failed\")\n",
    "    \n",
    "model = ModelWrapper('/tmp/lowered_convolution_node.onnx')\n",
    "input_dict = {\"global_in\": x_np}\n",
    "y_produced = oxe.execute_onnx(model, input_dict)[\"global_out\"]\n",
    "print(\"Lowered conv node:\")   \n",
    "if (y_produced==y_np).all():\n",
    "    print(\"Test passed\")\n",
    "else:\n",
    "    print(\"Test failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 2, 2)\n",
      "[[[[1 2]\n",
      "   [3 4]]\n",
      "\n",
      "  [[5 6]\n",
      "   [7 8]]]]\n"
     ]
    }
   ],
   "source": [
    "t = np.array([\n",
    "    [\n",
    "        [\n",
    "            [1,2],[3,4]\n",
    "        ],\n",
    "        [\n",
    "            [5,6],[7,8]\n",
    "        ]\n",
    "    ]\n",
    "])\n",
    "\n",
    "print(\"{}\\n{}\" .format(t.shape,t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 6],\n",
       "       [7, 8]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0][1][:][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
