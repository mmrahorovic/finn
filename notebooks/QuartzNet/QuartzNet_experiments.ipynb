{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "When performing the lowering transformation, the process gets killed. You can verify this yourself by running the script below. You could also run the same code in an interactive terminal, which gives the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.visualization import showInNetron\n",
    "import onnx\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "\n",
    "file_name = \"quartznet_modified_4d_inferred_shapes.onnx\"\n",
    "model = ModelWrapper(file_name)\n",
    "#import pdb; pdb.set_trace()\n",
    "\n",
    "model = model.transform(LowerConvsToMatMul(), make_deepcopy=True, cleanup=False, fix_float64=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think it is because the model is too large. When I added some print statements in the lower_convs_to_matmul.py file, I noticed that the process stopped at line model.transform(InferShapes()). As each Conv node gets replaced by 3-4 other nodes, the model grows by 171*3.5=600 nodes. In fact, the exact number of nodes is 1350 after the transformation (originally it was 931). \\\n",
    "Now, we can perform the InferShapes transformation on the original model (931 nodes). As only 419 nodes are added, it raises the question whether we can lower the size by perhaps calling several transfransormations as RemoveUnusedTensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.infer_shapes import InferShapes\n",
    "\n",
    "file_name = \"quartznet_modified_4d_inferred_shapes.onnx\"\n",
    "model = ModelWrapper(file_name)\n",
    "\n",
    "model = model.transform(InferShapes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some clean up before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.infer_shapes import InferShapes\n",
    "from finn.transformation.general import RemoveUnusedTensors\n",
    "from finn.transformation.double_to_single_float import DoubleToSingleFloat\n",
    "from finn.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "\n",
    "file_name = \"quartznet_modified_4d_inferred_shapes.onnx\"\n",
    "model = ModelWrapper(file_name)\n",
    "\n",
    "model = model.transform(RemoveUnusedTensors())\n",
    "model = model.transform(DoubleToSingleFloat())\n",
    "\n",
    "# Important:\n",
    "# Comment line 248 (model=model.transform(InferShapes()) in lower_convs_to_matmul \n",
    "# before running the line below). Save the file and restart the kernel.\n",
    "model = model.transform(LowerConvsToMatMul())\n",
    "\n",
    "model = model.transform(RemoveUnusedTensors())\n",
    "model = model.transform(DoubleToSingleFloat())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I cannot save the model, nor perform the InferShapes transformation without the script getting stopped. You can verify this by running the lines below. Another transformation, like MoveAddPastMul, also seems to fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Message ONNX_REL_1_7.ModelProto exceeds maximum protobuf size of 2GB: 3381363977",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-341a7f764256>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"modified_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/workspace/finn-base/src/finn/core/modelwrapper.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;34m\"\"\"Saves the wrapper ONNX ModelProto into a file with given name.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_proto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0manalysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalysis_fxn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/onnx/__init__.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(proto, f, format)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mproto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrite_external_data_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_serialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m     \u001b[0m_save_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/onnx/__init__.py\u001b[0m in \u001b[0;36m_serialize\u001b[0;34m(proto)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SerializeToString'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Message ONNX_REL_1_7.ModelProto exceeds maximum protobuf size of 2GB: 3381363977"
     ]
    }
   ],
   "source": [
    "model = model.save(\"modified_\"+file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.transform(InferShapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.streamline.reorder import MoveAddPastMul\n",
    "model = model.transform(MoveAddPastMul())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to fix this?\n",
    "A suggestion is made here: https://github.com/microsoft/onnxruntime/issues/4707 .\\\n",
    "Specifying a higher version of onnxruntime and onnx in the requirements.txt file does not solve the problem unfortunately, because the newer versions of onnxruntime are not found. I think this should be relatively easy to solve, but I do not know how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another problem would then remain: the model is too big for the InferShapes() transform. At least, that is what I identify as the problem. Perhaps I am missing something here. \\\n",
    "For now, the quickest solution is to extract a subgraph from the original graph, perform the lowering transformation, and verify whether the modified graph gives the same result as before the transformation. \\\n",
    "As other transformations also might not run, the transformations can be performed on subgraphs (on each repetitive structure in turn) instead of on the complete model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
