{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "municipal-owner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from finn.util.visualization import showInNetron\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "import onnx\n",
    "\n",
    "\n",
    "def generate_model(perm, default_data_layout):\n",
    "    if perm == [0, 3, 1, 2]:\n",
    "        in_shape = [1, 128, 1, 256]\n",
    "        out_shape = [1, 256, 128, 1]\n",
    "        data_layout = 'NCHW'\n",
    "    if perm == [0, 2, 3, 1]:\n",
    "        in_shape = [1, 256, 128, 1]\n",
    "        out_shape = [1, 128, 1, 256]\n",
    "        data_layout = 'NHWC'   \n",
    "    \n",
    "    Transpose1_node = onnx.helper.make_node(\n",
    "        \"Transpose\",\n",
    "        inputs = ['in_transpose1'],\n",
    "        outputs = ['out_transpose1'],\n",
    "        perm = perm\n",
    "    )\n",
    "\n",
    "    Transpose2_node = onnx.helper.make_node(\n",
    "        \"Transpose\",\n",
    "        inputs = ['in_transpose2'],\n",
    "        outputs = ['out_transpose2'],\n",
    "        perm = perm\n",
    "    )\n",
    "\n",
    "    if default_data_layout is True and data_layout == 'NCHW': # meaning that we will not set the data_layout attribute\n",
    "        Multithreshold1_node = onnx.helper.make_node(\n",
    "            \"MultiThreshold\",\n",
    "            inputs = ['out_transpose1', 'in2_multithreshold1'],\n",
    "            outputs = ['out_multithreshold1'],\n",
    "            domain = 'finn.custom_op.general',\n",
    "            out_dtype = 'UINT4'\n",
    "        )\n",
    "\n",
    "        Multithreshold2_node = onnx.helper.make_node(\n",
    "            \"MultiThreshold\",\n",
    "            inputs = ['out_transpose2', 'in2_multithreshold2'],\n",
    "            outputs = ['out_multithreshold2'],\n",
    "            domain = 'finn.custom_op.general',\n",
    "            out_dtype = 'UINT4'    \n",
    "        )\n",
    "    else: # we set the data_layout attribute\n",
    "        Multithreshold1_node = onnx.helper.make_node(\n",
    "            \"MultiThreshold\",\n",
    "            inputs = ['out_transpose1', 'in2_multithreshold1'],\n",
    "            outputs = ['out_multithreshold1'],\n",
    "            domain = 'finn.custom_op.general',\n",
    "            out_dtype = 'UINT4',\n",
    "            data_layout = data_layout\n",
    "        )\n",
    "\n",
    "        Multithreshold2_node = onnx.helper.make_node(\n",
    "            \"MultiThreshold\",\n",
    "            inputs = ['out_transpose2', 'in2_multithreshold2'],\n",
    "            outputs = ['out_multithreshold2'],\n",
    "            domain = 'finn.custom_op.general',\n",
    "            out_dtype = 'UINT4',    \n",
    "            data_layout = data_layout\n",
    "        )       \n",
    "\n",
    "    Add1_node = onnx.helper.make_node(\n",
    "        \"Add\",\n",
    "        inputs = ['out_multithreshold1', 'out_multithreshold2'],\n",
    "        outputs = ['out_add1']\n",
    "    )\n",
    "    \n",
    "    in_transpose1 = onnx.helper.make_tensor_value_info('in_transpose1', onnx.TensorProto.FLOAT, in_shape)\n",
    "    in_transpose2 = onnx.helper.make_tensor_value_info('in_transpose2', onnx.TensorProto.FLOAT, in_shape)\n",
    "    out_add1 = onnx.helper.make_tensor_value_info('out_add1', onnx.TensorProto.FLOAT, out_shape)\n",
    "\n",
    "    out_transpose1 = onnx.helper.make_tensor_value_info('out_transpose1', onnx.TensorProto.FLOAT, out_shape)\n",
    "    out_transpose2 = onnx.helper.make_tensor_value_info('out_transpose2', onnx.TensorProto.FLOAT, out_shape)\n",
    "    out_multithreshold1 = onnx.helper.make_tensor_value_info('out_multithreshold1', onnx.TensorProto.FLOAT, out_shape)\n",
    "    out_multithreshold2 = onnx.helper.make_tensor_value_info('out_multithreshold2', onnx.TensorProto.FLOAT, out_shape)\n",
    "\n",
    "    in2_multithreshold1 = onnx.helper.make_tensor_value_info('in2_multithreshold1', onnx.TensorProto.FLOAT, [256, 15])\n",
    "    in2_multithreshold2 = onnx.helper.make_tensor_value_info('in2_multithreshold2', onnx.TensorProto.FLOAT, [256, 15])\n",
    "\n",
    "    graph = onnx.helper.make_graph(\n",
    "        nodes = [\n",
    "            Transpose1_node,\n",
    "            Transpose2_node,\n",
    "            Multithreshold1_node,\n",
    "            Multithreshold2_node,\n",
    "            Add1_node\n",
    "        ],\n",
    "        name = \"test_graph\",\n",
    "        inputs = [in_transpose1, in_transpose2],\n",
    "        outputs = [out_add1],\n",
    "        value_info = [\n",
    "            out_transpose1,\n",
    "            out_transpose2,\n",
    "            out_multithreshold1,\n",
    "            out_multithreshold2,\n",
    "            in2_multithreshold1,\n",
    "            in2_multithreshold2\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    onnx_model = onnx.helper.make_model(graph, producer_name=\"test_model\")\n",
    "    model = ModelWrapper(onnx_model)\n",
    "\n",
    "    mt_weights = np.random.randint(low=-1000, high=1000, size=[256,15])\n",
    "    mt_weights = np.sort(mt_weights, 1)\n",
    "    model.set_initializer('in2_multithreshold1', mt_weights)\n",
    "    model.set_initializer('in2_multithreshold2', mt_weights)\n",
    "\n",
    "    model.save(\"/tmp/test_move_transpose_past_mt.onnx\")\n",
    "    showInNetron(\"/tmp/test_move_transpose_past_mt.onnx\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "answering-disney",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving '/tmp/test_move_transpose_past_mt.onnx' at http://0.0.0.0:8081\n"
     ]
    }
   ],
   "source": [
    "perm = [[0, 3, 1, 2], [0, 2, 3, 1]]\n",
    "model = generate_model(perm[1], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "square-colony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transpose\n",
      "Transpose\n",
      "MultiThreshold\n",
      "MultiThreshold\n",
      "Add\n",
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/tmp/test_move_transpose_past_mt_modified.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://0.0.0.0:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f863619ff98>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move transpose past multithreshold\n",
    "from finn.util.basic import get_by_name\n",
    "from finn.custom_op.registry import getCustomOp\n",
    "from finn.transformation.general import SortGraph\n",
    "import finn.core.data_layout as DataLayout\n",
    "\n",
    "model = ModelWrapper(\"/tmp/test_move_transpose_past_mt.onnx\")\n",
    "\n",
    "graph = model.graph\n",
    "graph_modified = False\n",
    "for n in graph.node:\n",
    "    print(n.op_type)\n",
    "    if (n.op_type == 'Transpose' and not model.is_fork_node(n)):\n",
    "        consumer = model.find_consumer(n.output[0])\n",
    "        if (consumer is not None and consumer.op_type == 'MultiThreshold' and not model.is_fork_node(consumer)):\n",
    "            perm = get_by_name(n.attribute, 'perm', 'name')\n",
    "            perm = onnx.helper.get_attribute_value(perm)\n",
    "            nhwc_to_nchw = [0, 3, 1, 2]\n",
    "            nchw_to_nhwc = [0, 2, 3, 1]\n",
    "            mt_inst = getCustomOp(consumer)\n",
    "            mt_data_layout = mt_inst.get_nodeattr('data_layout')\n",
    "\n",
    "            # Set new attribute\n",
    "            if mt_data_layout == 'NCHW':\n",
    "                if perm == nhwc_to_nchw:\n",
    "                    # Transpose(NHWC) -> NCHW. As we move the transpose node after the multithreshold node,\n",
    "                    # the input tensor will be in NHWC format\n",
    "                    new_data_layout = 'NHWC'\n",
    "                    new_tensor_data_layout = DataLayout.NHWC\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            elif mt_data_layout == 'NHWC':\n",
    "                if perm == nchw_to_nhwc:\n",
    "                    # Transpose(NCHW) -> NHWC. As we move the transpose node after the multithreshold node,\n",
    "                    # the input tensor will be in NCHW format\n",
    "                    new_data_layout = 'NCHW'\n",
    "                    new_tensor_data_layout = DataLayout.NCHW\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            # Update the node attribute data_layout\n",
    "            mt_inst.set_nodeattr('data_layout', new_data_layout)\n",
    "\n",
    "            # Now we will rewire the nodes accordingly\n",
    "            transpose_in = n.input[0]\n",
    "            transpose_out = n.output[0]\n",
    "            multithreshold_out = consumer.output[0]\n",
    "            transpose_in_shape = model.get_tensor_shape(transpose_in)\n",
    "            transpose_in_layout = model.get_tensor_layout(transpose_in)\n",
    "\n",
    "            # First we move the multithreshold node in front of the transpose node\n",
    "            consumer.input[0] = transpose_in\n",
    "            consumer.output[0] = transpose_out\n",
    "            # We must change the shape of transpose_out tensor to match the shape of the input tensor\n",
    "            model.set_tensor_shape(transpose_out, transpose_in_shape)\n",
    "            model.set_tensor_layout(transpose_out, new_tensor_data_layout)\n",
    "            model.set_tensor_layout(transpose_in, new_tensor_data_layout)\n",
    "\n",
    "            # Finally, we ensure the right tensors are connected to the Transpose node\n",
    "            n.input[0] = consumer.output[0]\n",
    "            n.output[0] = multithreshold_out\n",
    "\n",
    "# We must ensure the graph is sorted, because we reordered the nodes.\n",
    "model = model.transform(SortGraph())    \n",
    "    \n",
    "model.save(\"/tmp/test_move_transpose_past_mt_modified.onnx\")\n",
    "showInNetron(\"/tmp/test_move_transpose_past_mt_modified.onnx\")                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "restricted-witness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from finn.util.basic import gen_finn_dt_tensor\n",
    "from finn.core.datatype import DataType\n",
    "import finn.core.onnx_exec as oxe\n",
    "\n",
    "model = ModelWrapper(\"/tmp/test_move_transpose_past_mt.onnx\")\n",
    "\n",
    "# Create input data\n",
    "input0_tensor_name = model.graph.input[0].name\n",
    "input1_tensor_name = model.graph.input[1].name\n",
    "\n",
    "# Note: it is assumed that both tensors have the same shape and data type\n",
    "input_shape = model.get_tensor_shape(input0_tensor_name)\n",
    "input_dtype = model.get_tensor_datatype(input0_tensor_name)\n",
    "input_val = gen_finn_dt_tensor(input_dtype, input_shape)\n",
    "input_dict = {}\n",
    "input_dict[input0_tensor_name] = input_val\n",
    "input_dict[input1_tensor_name] = input_val\n",
    "\n",
    "model = ModelWrapper(\"/tmp/test_move_transpose_past_mt.onnx\")\n",
    "model_transformed = ModelWrapper(\"/tmp/test_move_transpose_past_mt_modified.onnx\")\n",
    "is_same = oxe.compare_execution(model, model_transformed, input_dict)\n",
    "\n",
    "print(is_same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "surrounded-laugh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "# Check if order changed\n",
    "node0_input0_model = model.find_consumer(model.graph.input[0].name).op_type\n",
    "node1_input1_model = model.find_consumer(model.graph.input[1].name).op_type\n",
    "node0_input0_model_transformed = model_transformed.find_consumer(model_transformed.graph.input[0].name).op_type\n",
    "node1_input1_model_transformed = model_transformed.find_consumer(model_transformed.graph.input[1].name).op_type\n",
    "assert node0_input0_model != node0_input0_model_transformed\n",
    "assert node1_input1_model != node1_input1_model_transformed\n",
    "mt0_input = model_transformed.graph.node[0].input[0]\n",
    "mt1_input = model_transformed.graph.node[1].input[0]\n",
    "if perm == [0, 3, 1, 2]:\n",
    "    assert model_transformed.get_tensor_layout(mt0_input) == DataLayout.NHWC\n",
    "    assert model_transformed.get_tensor_layout(mt1_input) == DataLayout.NHWC\n",
    "if perm == [0, 2, 3, 1]:\n",
    "    assert model_transformed.get_tensor_layout(mt0_input) == DataLayout.NCHW\n",
    "    assert model_transformed.get_tensor_layout(mt1_input) == DataLayout.NCHW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-asian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-beads",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-respondent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-verification",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "separated-result",
   "metadata": {},
   "source": [
    "# Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-video",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoveTransposePastMultiThreshold(Transformation):\n",
    "    \"\"\"Moves Transpose nodes past MultiThreshold nodes on linear segments\n",
    "    of the graph.\"\"\"\n",
    "\n",
    "    def apply(self, model):\n",
    "        graph = model.graph\n",
    "        graph_modified = False\n",
    "        for n in graph.node:\n",
    "            if n.op_type == \"Transpose\" and not model.is_fork_node(n):\n",
    "                consumer = model.find_consumer(n.output[0])\n",
    "                if (\n",
    "                    consumer is not None\n",
    "                    and consumer.op_type == \"MultiThreshold\"\n",
    "                    and not model.is_fork_node(consumer)\n",
    "                ):\n",
    "                    perm = get_by_name(n.attribute, \"perm\", \"name\")\n",
    "                    perm = oh.get_attribute_value(perm)\n",
    "                    nhwc_to_nchw = [0, 3, 1, 2]\n",
    "                    nchw_to_nhwc = [0, 2, 3, 1]\n",
    "                    mt_inst = getCustomOp(consumer)\n",
    "                    mt_data_layout = mt_inst.get_nodeattr(\"data_layout\")\n",
    "\n",
    "                    # Set new attribute\n",
    "                    if mt_data_layout == \"NCHW\":\n",
    "                        if perm == nhwc_to_nchw:\n",
    "                            # Transpose(NHWC) -> NCHW. As we move the transpose node\n",
    "                            # after the multithreshold node, the input tensor\n",
    "                            # will be in NHWC format\n",
    "                            new_data_layout = \"NHWC\"\n",
    "                            new_tensor_data_layout = DataLayout.NHWC\n",
    "                            graph_modified = True\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    elif mt_data_layout == \"NHWC\":\n",
    "                        if perm == nchw_to_nhwc:\n",
    "                            # Transpose(NCHW) -> NHWC. As we move the transpose node\n",
    "                            # after the multithreshold node, the input tensor\n",
    "                            # will be in NCHW format\n",
    "                            new_data_layout = \"NCHW\"\n",
    "                            new_tensor_data_layout = DataLayout.NCHW\n",
    "                            graph_modified = True\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Update the node attribute data_layout\n",
    "                    mt_inst.set_nodeattr(\"data_layout\", new_data_layout)\n",
    "\n",
    "                    # Now we will rewire the nodes accordingly\n",
    "                    transpose_in = n.input[0]\n",
    "                    transpose_out = n.output[0]\n",
    "                    multithreshold_out = consumer.output[0]\n",
    "                    transpose_in_shape = model.get_tensor_shape(transpose_in)\n",
    "\n",
    "                    # Move the multithreshold node in front of the transpose node\n",
    "                    consumer.input[0] = transpose_in\n",
    "                    consumer.output[0] = transpose_out\n",
    "                    # Change the shape of transpose_out tensor to match the shape\n",
    "                    # of the input tensor\n",
    "                    model.set_tensor_shape(transpose_out, transpose_in_shape)\n",
    "                    model.set_tensor_layout(transpose_out, new_tensor_data_layout)\n",
    "                    model.set_tensor_layout(transpose_in, new_tensor_data_layout)\n",
    "\n",
    "                    # Ensure the right tensors are connected to the Transpose node\n",
    "                    n.input[0] = consumer.output[0]\n",
    "                    n.output[0] = multithreshold_out\n",
    "\n",
    "        # We must ensure the graph is sorted, because we reordered the nodes\n",
    "        if graph_modified:\n",
    "            model = model.transform(SortGraph(), make_deepcopy=False, cleanup=False)\n",
    "\n",
    "        return (model, graph_modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-database",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "import numpy as np\n",
    "from onnx import helper as oh\n",
    "from onnx import TensorProto\n",
    "\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "import finn.core.data_layout as DataLayout\n",
    "from finn.transformation.streamline.reorder import MoveTransposePastMultiThreshold\n",
    "from finn.util.basic import gen_finn_dt_tensor\n",
    "import finn.core.onnx_exec as oxe\n",
    "\n",
    "def create_model(permutation, default_data_layout):\n",
    "    if permutation == [0, 3, 1, 2]:\n",
    "        in_shape = [1, 128, 1, 256]\n",
    "        out_shape = [1, 256, 128, 1]\n",
    "        data_layout = 'NCHW'\n",
    "    if permutation == [0, 2, 3, 1]:\n",
    "        in_shape = [1, 256, 128, 1]\n",
    "        out_shape = [1, 128, 1, 256]\n",
    "        data_layout = 'NHWC'\n",
    "\n",
    "    Transpose1_node = oh.make_node(\n",
    "        \"Transpose\",\n",
    "        inputs = ['in_transpose1'],\n",
    "        outputs = ['out_transpose1'],\n",
    "        perm = permutation\n",
    "    )\n",
    "\n",
    "    Transpose2_node = oh.make_node(\n",
    "        \"Transpose\",\n",
    "        inputs = ['in_transpose2'],\n",
    "        outputs = ['out_transpose2'],\n",
    "        perm = permutation\n",
    "    )\n",
    "\n",
    "    if default_data_layout is True and data_layout == 'NCHW': # meaning that we will not set the data_layout attribute\n",
    "        Multithreshold1_node = oh.make_node(\n",
    "            \"MultiThreshold\",\n",
    "            inputs = ['out_transpose1', 'in2_multithreshold1'],\n",
    "            outputs = ['out_multithreshold1'],\n",
    "            domain = 'finn.custom_op.general',\n",
    "            out_dtype = 'UINT4'\n",
    "        )\n",
    "\n",
    "        Multithreshold2_node = oh.make_node(\n",
    "            \"MultiThreshold\",\n",
    "            inputs = ['out_transpose2', 'in2_multithreshold2'],\n",
    "            outputs = ['out_multithreshold2'],\n",
    "            domain = 'finn.custom_op.general',\n",
    "            out_dtype = 'UINT4'\n",
    "        )\n",
    "    else: # we set the data_layout attribute\n",
    "        Multithreshold1_node = oh.make_node(\n",
    "            \"MultiThreshold\",\n",
    "            inputs = ['out_transpose1', 'in2_multithreshold1'],\n",
    "            outputs = ['out_multithreshold1'],\n",
    "            domain = 'finn.custom_op.general',\n",
    "            out_dtype = 'UINT4',\n",
    "            data_layout = data_layout\n",
    "        )\n",
    "\n",
    "        Multithreshold2_node = oh.make_node(\n",
    "            \"MultiThreshold\",\n",
    "            inputs = ['out_transpose2', 'in2_multithreshold2'],\n",
    "            outputs = ['out_multithreshold2'],\n",
    "            domain = 'finn.custom_op.general',\n",
    "            out_dtype = 'UINT4',\n",
    "            data_layout = data_layout\n",
    "        )\n",
    "\n",
    "    Add1_node = oh.make_node(\n",
    "        \"Add\",\n",
    "        inputs = ['out_multithreshold1', 'out_multithreshold2'],\n",
    "        outputs = ['out_add1']\n",
    "    )\n",
    "\n",
    "    in_transpose1 = oh.make_tensor_value_info('in_transpose1', TensorProto.FLOAT, in_shape)\n",
    "    in_transpose2 = oh.make_tensor_value_info('in_transpose2', TensorProto.FLOAT, in_shape)\n",
    "    out_add1 = oh.make_tensor_value_info('out_add1', TensorProto.FLOAT, out_shape)\n",
    "\n",
    "    out_transpose1 = oh.make_tensor_value_info('out_transpose1', TensorProto.FLOAT, out_shape)\n",
    "    out_transpose2 = oh.make_tensor_value_info('out_transpose2', TensorProto.FLOAT, out_shape)\n",
    "    out_multithreshold1 = oh.make_tensor_value_info('out_multithreshold1', TensorProto.FLOAT, out_shape)\n",
    "    out_multithreshold2 = oh.make_tensor_value_info('out_multithreshold2', TensorProto.FLOAT, out_shape)\n",
    "\n",
    "    in2_multithreshold1 = oh.make_tensor_value_info('in2_multithreshold1', TensorProto.FLOAT, [256, 15])\n",
    "    in2_multithreshold2 = oh.make_tensor_value_info('in2_multithreshold2', TensorProto.FLOAT, [256, 15])\n",
    "\n",
    "    graph = oh.make_graph(\n",
    "        nodes = [\n",
    "            Transpose1_node,\n",
    "            Transpose2_node,\n",
    "            Multithreshold1_node,\n",
    "            Multithreshold2_node,\n",
    "            Add1_node\n",
    "        ],\n",
    "        name = \"test_graph\",\n",
    "        inputs = [in_transpose1, in_transpose2],\n",
    "        outputs = [out_add1],\n",
    "        value_info = [\n",
    "            out_transpose1,\n",
    "            out_transpose2,\n",
    "            out_multithreshold1,\n",
    "            out_multithreshold2,\n",
    "            in2_multithreshold1,\n",
    "            in2_multithreshold2\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    onnx_model = oh.make_model(graph, producer_name=\"test_model\")\n",
    "    model = ModelWrapper(onnx_model)\n",
    "\n",
    "    mt_weights = np.random.randint(low=-1000, high=1000, size=[256,15])\n",
    "    mt_weights = np.sort(mt_weights, 1)\n",
    "    model.set_initializer('in2_multithreshold1', mt_weights)\n",
    "    model.set_initializer('in2_multithreshold2', mt_weights)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# permutation of transpose node\n",
    "@pytest.mark.parametrize(\"perm\", [[0, 3, 1, 2], [0, 2, 3, 1]])\n",
    "# default data layout variable\n",
    "@pytest.mark.parametrize(\"default_data_layout\", [True, False])\n",
    "def test_move_transpose_past_multithreshold(perm, default_data_layout):\n",
    "    model = create_model(perm, default_data_layout)\n",
    "\n",
    "    # Create input data\n",
    "    input0_tensor_name = model.graph.input[0].name\n",
    "    input1_tensor_name = model.graph.input[1].name\n",
    "\n",
    "    # Note: it is assumed that both tensors have the same shape and data type\n",
    "    input_shape = model.get_tensor_shape(input0_tensor_name)\n",
    "    input_dtype = model.get_tensor_datatype(input0_tensor_name)\n",
    "    input_val = gen_finn_dt_tensor(input_dtype, input_shape)\n",
    "    input_dict = {}\n",
    "    input_dict[input0_tensor_name] = input_val\n",
    "    input_dict[input1_tensor_name] = input_val\n",
    "\n",
    "    model_transformed = model.transform(MoveTransposePastMultiThreshold())\n",
    "\n",
    "    assert oxe.compare_execution(model, model_transformed, input_dict)\n",
    "\n",
    "    # Check if order changed\n",
    "    node0_input0_model = model.find_consumer(model.graph.input[0].name).op_type\n",
    "    node1_input1_model = model.find_consumer(model.graph.input[1].name).op_type\n",
    "    node0_input0_model_transformed = model_transformed.find_consumer(model_transformed.graph.input[0].name).op_type\n",
    "    node1_input1_model_transformed = model_transformed.find_consumer(model_transformed.graph.input[1].name).op_type\n",
    "    assert node0_input0_model != node0_input0_model_transformed\n",
    "    assert node1_input1_model != node1_input1_model_transformed\n",
    "\n",
    "    # Check if data_layout is set correctly\n",
    "    mt0_input = model_transformed.graph.node[0].input[0]\n",
    "    mt1_input = model_transformed.graph.node[1].input[0]\n",
    "    mt0_output = model_transformed.graph.node[0].output[0]\n",
    "    mt1_output = model_transformed.graph.node[1].output[0]\n",
    "    if perm == [0, 3, 1, 2]:\n",
    "        assert model_transformed.get_tensor_layout(mt0_input) == DataLayout.NHWC\n",
    "        assert model_transformed.get_tensor_layout(mt1_input) == DataLayout.NHWC\n",
    "        assert model_transformed.get_tensor_layout(mt0_output) == DataLayout.NHWC\n",
    "        assert model_transformed.get_tensor_layout(mt1_output) == DataLayout.NHWC\n",
    "    if perm == [0, 2, 3, 1]:\n",
    "        assert model_transformed.get_tensor_layout(mt0_input) == DataLayout.NCHW\n",
    "        assert model_transformed.get_tensor_layout(mt1_input) == DataLayout.NCHW\n",
    "        assert model_transformed.get_tensor_layout(mt0_output) == DataLayout.NCHW\n",
    "        assert model_transformed.get_tensor_layout(mt1_output) == DataLayout.NCHW\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
